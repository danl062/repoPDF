# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-jWaskl_x_vj1LtNtp2VTVbWVKg65zx
"""


import os
from langchain import PromptTemplate
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.deeplake import DeepLake
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyMuPDFLoader
from langchain.chat_models.openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferWindowMemory
from langchain.callbacks import get_openai_callback

import gradio as gr



OPENAI_API_KEY = "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"
TEXT_VECTORSTORE_PATH = "Devoir_1.pdf"
CHARACTER_SPLITTER_CHUNK_SIZE = 75
OPENAI_EMBEDDINGS_CHUNK_SIZE = 16

os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY

class Retriever:
    def __init__(self):
        self.text_retriever = None
        self.text_deeplake_schema = None
        self.embeddings = None
        self.memory = ConversationBufferWindowMemory(k=2, return_messages=True)

    def create_and_add_embeddings(self, file):
        os.makedirs("data", exist_ok=True)

        self.embeddings = OpenAIEmbeddings(
            openai_api_key=OPENAI_API_KEY,
            chunk_size=OPENAI_EMBEDDINGS_CHUNK_SIZE,
        )

        loader = PyMuPDFLoader(file)
        documents = loader.load()
        text_splitter = CharacterTextSplitter(
            chunk_size=CHARACTER_SPLITTER_CHUNK_SIZE,
            chunk_overlap=0,
        )
        docs = text_splitter.split_documents(documents)

        self.text_deeplake_schema = DeepLake(
            dataset_path=TEXT_VECTORSTORE_PATH,
            embedding_function=self.embeddings,
            overwrite=True,
        )

        self.text_deeplake_schema.add_documents(docs)

        self.text_retriever = self.text_deeplake_schema.as_retriever(
            search_type="similarity"
        )
        self.text_retriever.search_kwargs["distance_metric"] = "cos"
        self.text_retriever.search_kwargs["fetch_k"] = 15
        self.text_retriever.search_kwargs["maximal_marginal_relevance"] = True
        self.text_retriever.search_kwargs["k"] = 3

    def retrieve_text(self, query):
        self.text_deeplake_schema = DeepLake(
            dataset_path=TEXT_VECTORSTORE_PATH,
            read_only=True,
            embedding_function=self.embeddings,
        )

        prompt_template = """You are an intelligent AI which analyses text from documents and
        answers the user's questions. Please answer in as much detail as possible, so that the user does not have to
        revisit the document. If you don't know the answer, say that you don't know, and avoid making up things.
        {context}
        Question: {question}
        Answer:
        """

        PROMPT = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )
        chain_type_kwargs = {"prompt": PROMPT}

        model = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            openai_api_key=OPENAI_API_KEY,
        )

        qa = RetrievalQA.from_chain_type(
            llm=model,
            chain_type="stuff",
            retriever=self.text_retriever,
            return_source_documents=False,
            verbose=False,
            chain_type_kwargs=chain_type_kwargs,
            memory=self.memory,
        )

        response = qa({"query": query})
        return response["result"]

class Controller:
    def __init__(self):
        self.retriever = None
        self.query = ""

    def embed_document(self, file):
        if file is not None:
            self.retriever = Retriever()
            self.retriever.create_and_add_embeddings(file.name)

    def retrieve(self, query):
        texts = self.retriever.retrieve_text(query)
        return texts

!pip install controller

import os
from controller import Controller
import gradio as gr

os.environ["TOKENIZERS_PARALLELISM"] = "false"
colors = ["#64A087", "green", "black"]

CSS = """
#question input {
    font-size: 16px;
}
#app-title {
    width: 100%;
    margin: auto;
}
#url-textbox {
    padding: 0 !important;
}
#short-upload-box .w-full {
    min-height: 10rem !important;
}

#select-a-file {
    display: block;
    width: 100%;
}
#file-clear {
    padding-top: 2px !important;
    padding-bottom: 2px !important;
    padding-left: 8px !important;
    padding-right: 8px !important;
	margin-top: 10px;
}
.gradio-container .gr-button-primary {
    background: linear-gradient(180deg, #CDF9BE 0%, #AFF497 100%);
    border: 1px solid #B0DCCC;
    border-radius: 8px;
    color: #1B8700;
}
.gradio-container.dark button#submit-button {
    background: linear-gradient(180deg, #CDF9BE 0%, #AFF497 100%);
    border: 1px solid #B0DCCC;
    border-radius: 8px;
    color: #1B8700
}
table.gr-samples-table tr td {
    border: none;
    outline: none;
}
table.gr-samples-table tr td:first-of-type {
    width: 0%;
}
div#short-upload-box div.absolute {
    display: none !important;
}
gradio-app > div > div > div > div.w-full > div, .gradio-app > div > div > div > div.w-full > div {
    gap: 0px 2%;
}
gradio-app div div div div.w-full, .gradio-app div div div div.w-full {
    gap: 0px;
}
gradio-app h2, .gradio-app h2 {
    padding-top: 10px;
}
#answer {
    overflow-y: scroll;
    color: white;
    background: #666;
    border-color: #666;
    font-size: 20px;
    font-weight: bold;
}
#answer span {
    color: white;
}
#answer textarea {
    color:white;
    background: #777;
    border-color: #777;
    font-size: 18px;
}
#url-error input {
    color: red;
}
"""
# TODO: Copy
controller = Controller()


def process_pdf(file):
    if file is not None:
        controller.embed_document(file)
    return (
        gr.update(visible=True),
        gr.update(visible=True),
        gr.update(visible=True),
        gr.update(visible=True),
    )


def respond(message, history):
    botmessage = controller.retrieve(message)
    history.append((message, botmessage))
    return "", history


def clear_everything():
    return (None, None, None)


with gr.Blocks(css=CSS, title="") as demo:
    gr.Markdown("# AskPDF ", elem_id="app-title")
    gr.Markdown("## Upload a PDF and Ask Questions!", elem_id="select-a-file")
    gr.Markdown(
        "Drop an interesting PDF and ask questions about it!",
        elem_id="select-a-file",
    )
    with gr.Row():
        with gr.Column(scale=3):
            upload = gr.File(label="Upload PDF", type="file")
            with gr.Row():
                clear_button = gr.Button("Clear", variant="secondary")

        with gr.Column(scale=6):
            chatbot = gr.Chatbot()
            with gr.Row().style(equal_height=True):
                with gr.Column(scale=8):
                    question = gr.Textbox(
                        show_label=False,
                        placeholder="e.g. What is the document about?",
                        lines=1,
                        max_lines=1,
                    ).style(container=False)
                with gr.Column(scale=1, min_width=60):
                    submit_button = gr.Button(
                        "Ask me ðŸ¤–", variant="primary", elem_id="submit-button"
                    )

    upload.change(
        fn=process_pdf,
        inputs=[upload],
        outputs=[
            question,
            clear_button,
            submit_button,
            chatbot,
        ],
        api_name="upload",
    )
    question.submit(respond, [question, chatbot], [question, chatbot])
    submit_button.click(respond, [question, chatbot], [question, chatbot])
    clear_button.click(
        fn=clear_everything,
        inputs=[],
        outputs=[upload, question, chatbot],
        api_name="clear",
    )

if __name__ == "__main__":
    demo.launch(enable_queue=False, share=False)

